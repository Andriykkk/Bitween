"make so that in mlp with experts it also train individual layers that dont get enought attention"


 Expert-Aware Routing (More Complex)
This would require:
Track expert activation during caching:
Monitor which experts activate for each sample
Build a map: {expert_layer_name: [sample_indices]}
Train experts individually:
For each expert layer in MLP block
Use only samples that activate that expert
Ensure minimum samples per expert
Code changes needed:
Modify cache collection to track activations
Add expert detection logic
Train layers individually instead of block-level