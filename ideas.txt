[ ] add gradual quantisation
    [x] make training separate layers for mlp with experts
    [ ] add layer freezing to training
    [ ] in the end run training quantization for whole model to check everything and also try to increase freeze layers group size
    [ ] make usual algorithm that remove layers that have too high error and could not learn and some layers learn to lover bits
    [ ] later think about quantising with separate model look for outliers and model that find hyperparameters and look what layers to train (check usual model and hyerarhical reasoning model)
    [ ] make so that in mlp with experts it also train individual layers that dont get enought attention
[ ] quantise 70b model to 2 bit
[ ] quantise several neighbor blocks and move by one everytime like this blocks could learn to work with each other instead of blindly quantise each block
[ ] add evaluation before and after quantisation
[ ] add quantisation on cpu instead of gpu
[ ] add outliers to separate matrix
[ ] some matrices could be quantised with 1.58 bits 
[ ] add quantisation to 1 bit and perform with two 1 bit linear layers addition using bits functions instead of transforming back to 16bit(look at the pytorch graph for this)
